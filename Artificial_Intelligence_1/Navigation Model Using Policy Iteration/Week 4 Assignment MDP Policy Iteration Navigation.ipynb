{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6dccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "COURSE: Artificial Intelligence II\n",
    "Week 4 Assignment: Markov Decision Process: Environment Navigation Model Using Policy Iteration\n",
    "SEMESTER: Fall 2021\n",
    "NAME: Joe Cruz\n",
    "DATE: 26Nov2021\n",
    "\n",
    "The purpose of this algorithm is to simulate agent navigation through a simulated environment \n",
    "using Markov Decision Process (MDP) based algorithm in tandem with policy iteration. Additionally,\n",
    "a time study is performed using differently scaled environments.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b2fd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ns = { 'a' : {\\t'plan1' : [(0.2, 'a'), (0.3, 'b'), (0.3, 'c'), (0.2, 'd')],\\n                'plan2' : [(0.4, 'a'), (0.15, 'b'), (0.45, 'c')],\\n                'plan3' : [(0.2, 'a'), (0.5, 'b'), (0.3, 'c')],\\n                 },\\n      'b' : {\\t'plan1' : [(0.2, 'a'), (0.6, 'b'), (0.2, 'c'), (0.1, 'd')],\\n                'plan2' : [(0.6, 'a'), (0.2, 'b'), (0.1, 'c'), (0.1, 'd')],\\n                'plan3' : [(0.3, 'a'), (0.3, 'b'), (0.4, 'c')],\\n                },\\n        'c' : {\\t'plan1' : [(0.3, 'a'), (0.5, 'b'), (0.1, 'c'), (0.1, 'd')],\\n                'plan2' : [(0.5, 'a'), (0.3, 'b'), (0.1, 'c'), (0.1, 'd')],\\n                'plan3' : [(0.1, 'a'), (0.3, 'b'), (0.1, 'c'), (0.5, 'd')],\\n                },\\n    }\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "THIS BLOCK OF CODE IS FROM THE FOLLOWING REFERENCE:\n",
    "P. Norvig, et al., “mdp.py” [Online]. Available: https://github.com/aimacode/aima-python/blob/master/mdp.py. [Accessed: November 24, 2021]. [Source Code]. \n",
    "\n",
    "\n",
    "This was used and slightly modified to display utility values when the policy iteration is performed. \n",
    "Any changes made by Joe Cruz have 7 #'s and a comment regarding the change made. Otherwise the rest\n",
    "of this code was from the reference mentioned at the top.\n",
    "\n",
    "######\n",
    "\n",
    "Markov Decision Processes (Chapter 17)\n",
    "First we define an MDP, and the special case of a GridMDP, in which\n",
    "states are laid out in a 2-dimensional grid. We also represent a policy\n",
    "as a dictionary of {state: action} pairs, and a Utility function as a\n",
    "dictionary of {state: number} pairs. We then define the value_iteration\n",
    "and policy_iteration algorithms.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import vector_add, orientations, turn_right, turn_left, print_table\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [Page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "\n",
    "        self.init = init\n",
    "\n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001\n",
    "\n",
    "\n",
    "class MDP2(MDP):\n",
    "    \"\"\"\n",
    "    Inherits from MDP. Handles terminal states, and transitions to and from terminal states better.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions, reward=None, gamma=0.9):\n",
    "        MDP.__init__(self, init, actlist, terminals, transitions, reward, gamma=gamma)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1]. All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state). Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        reward = {}\n",
    "        states = set()\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                if grid[y][x]:\n",
    "                    states.add((x, y))\n",
    "                    reward[(x, y)] = grid[y][x]\n",
    "        self.states = states\n",
    "        actlist = orientations\n",
    "        transitions = {}\n",
    "        for s in states:\n",
    "            transitions[s] = {}\n",
    "            for a in actlist:\n",
    "                transitions[s][a] = self.calculate_T(s, a)\n",
    "        MDP.__init__(self, init, actlist=actlist,\n",
    "                     terminals=terminals, transitions=transitions,\n",
    "                     reward=reward, states=states, gamma=gamma)\n",
    "\n",
    "    def calculate_T(self, state, action):\n",
    "        if action:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, turn_right(action))),\n",
    "                    (0.1, self.go(state, turn_left(action)))]\n",
    "        else:\n",
    "            return [(0.0, state)]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        return self.transitions[state][action] if action else [(0.0, state)]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        \"\"\"Return the state that results from going in this direction.\"\"\"\n",
    "\n",
    "        state1 = vector_add(state, direction)\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "\n",
    "\n",
    "\"\"\" [Figure 17.1]\n",
    "A 4x3 grid environment that presents the agent with a sequential decision problem.\n",
    "\"\"\"\n",
    "#4x3\n",
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                                           [ -0.04, None, -0.04, -1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(3, 2), (3, 1)])\n",
    "# ______________________________________________________________________________\n",
    "\n",
    "\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    \"\"\"Solving an MDP by value iteration. [Figure 17.4]\"\"\"\n",
    "\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max(sum(p * U[s1] for (p, s1) in T(s, a))\n",
    "                                       for a in mdp.actions(s))\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta <= epsilon * (1 - gamma) / gamma:\n",
    "            return U\n",
    "\n",
    "\n",
    "def best_policy(mdp, U):\n",
    "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
    "    as a mapping from state to action. [Equation 17.4]\"\"\"\n",
    "\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        pi[s] = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "    return pi\n",
    "\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    \"\"\"The expected utility of doing a in state s, according to the MDP and U.\"\"\"\n",
    "\n",
    "    return sum(p * U[s1] for (p, s1) in mdp.T(s, a))\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    \"\"\"Solve an MDP by policy iteration [Figure 17.7]\"\"\"\n",
    "\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        #print(U)\n",
    "        for s in mdp.states:\n",
    "            a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            #######CHANGE MADE HERE: ADDED SEGMENT TO MAP THE UTILITY VALUES FOR EACH STATE TO \n",
    "            #######THEIR RESPECTIVE POSITIONS IN THE MATRIX ALSO MADE THEM PRINTED WHEN PROGRAM\n",
    "            #######IS RUN.\n",
    "            print(\"Computed utilities for this policy iteration:\")\n",
    "            grid_U = mdp.to_grid(U)\n",
    "            for entry in grid_U:\n",
    "                print(entry)\n",
    "            return pi\n",
    "\n",
    "\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n",
    "    utility, using an approximation (modified policy iteration).\"\"\"\n",
    "\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum(p * U[s1] for (p, s1) in T(s, pi[s]))\n",
    "    return U\n",
    "\n",
    "\n",
    "class POMDP(MDP):\n",
    "    \"\"\"A Partially Observable Markov Decision Process, defined by\n",
    "    a transition model P(s'|s,a), actions A(s), a reward function R(s),\n",
    "    and a sensor model P(e|s). We also keep track of a gamma value,\n",
    "    for use by algorithms. The transition and the sensor models\n",
    "    are defined as matrices. We also keep track of the possible states\n",
    "    and actions for each state. [Page 659].\"\"\"\n",
    "\n",
    "    def __init__(self, actions, transitions=None, evidences=None, rewards=None, states=None, gamma=0.95):\n",
    "        \"\"\"Initialize variables of the pomdp\"\"\"\n",
    "\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError('A POMDP must have 0 < gamma <= 1')\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # transition model cannot be undefined\n",
    "        self.t_prob = transitions or {}\n",
    "        if not self.t_prob:\n",
    "            print('Warning: Transition model is undefined')\n",
    "\n",
    "        # sensor model cannot be undefined\n",
    "        self.e_prob = evidences or {}\n",
    "        if not self.e_prob:\n",
    "            print('Warning: Sensor model is undefined')\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def remove_dominated_plans(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans.\n",
    "        This method finds all the lines contributing to the\n",
    "        upper surface and removes those which don't.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = [values[0]]\n",
    "        y1_max = max(val[1] for val in values)\n",
    "        tgt = values[0]\n",
    "        prev_b = 0\n",
    "        prev_ix = 0\n",
    "        while tgt[1] != y1_max:\n",
    "            min_b = 1\n",
    "            min_ix = 0\n",
    "            for i in range(prev_ix + 1, len(values)):\n",
    "                if values[i][0] - tgt[0] + tgt[1] - values[i][1] != 0:\n",
    "                    trans_b = (values[i][0] - tgt[0]) / (values[i][0] - tgt[0] + tgt[1] - values[i][1])\n",
    "                    if 0 <= trans_b <= 1 and trans_b > prev_b and trans_b < min_b:\n",
    "                        min_b = trans_b\n",
    "                        min_ix = i\n",
    "            prev_b = min_b\n",
    "            prev_ix = min_ix\n",
    "            tgt = values[min_ix]\n",
    "            best.append(tgt)\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def remove_dominated_plans_fast(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans using approximations.\n",
    "        Resamples the upper boundary at intervals of 100 and\n",
    "        finds the maximum values at these points.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = []\n",
    "        sr = 100\n",
    "        for i in range(sr + 1):\n",
    "            x = i / float(sr)\n",
    "            maximum = (values[0][1] - values[0][0]) * x + values[0][0]\n",
    "            tgt = values[0]\n",
    "            for value in values:\n",
    "                val = (value[1] - value[0]) * x + value[0]\n",
    "                if val > maximum:\n",
    "                    maximum = val\n",
    "                    tgt = value\n",
    "\n",
    "            if all(any(tgt != v) for v in best):\n",
    "                best.append(np.array(tgt))\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def generate_mapping(self, best, input_values):\n",
    "        \"\"\"Generate mappings after removing dominated plans\"\"\"\n",
    "\n",
    "        mapping = defaultdict(list)\n",
    "        for value in best:\n",
    "            for action in input_values:\n",
    "                if any(all(value == v) for v in input_values[action]):\n",
    "                    mapping[action].append(value)\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def max_difference(self, U1, U2):\n",
    "        \"\"\"Find maximum difference between two utility mappings\"\"\"\n",
    "\n",
    "        for k, v in U1.items():\n",
    "            sum1 = 0\n",
    "            for element in U1[k]:\n",
    "                sum1 += sum(element)\n",
    "            sum2 = 0\n",
    "            for element in U2[k]:\n",
    "                sum2 += sum(element)\n",
    "        return abs(sum1 - sum2)\n",
    "\n",
    "\n",
    "class Matrix:\n",
    "    \"\"\"Matrix operations class\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add(A, B):\n",
    "        \"\"\"Add two matrices A and B\"\"\"\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(A)):\n",
    "            row = []\n",
    "            for j in range(len(A[0])):\n",
    "                row.append(A[i][j] + B[i][j])\n",
    "            res.append(row)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def scalar_multiply(a, B):\n",
    "        \"\"\"Multiply scalar a to matrix B\"\"\"\n",
    "\n",
    "        for i in range(len(B)):\n",
    "            for j in range(len(B[0])):\n",
    "                B[i][j] = a * B[i][j]\n",
    "        return B\n",
    "\n",
    "    @staticmethod\n",
    "    def multiply(A, B):\n",
    "        \"\"\"Multiply two matrices A and B element-wise\"\"\"\n",
    "\n",
    "        matrix = []\n",
    "        for i in range(len(B)):\n",
    "            row = []\n",
    "            for j in range(len(B[0])):\n",
    "                row.append(B[i][j] * A[j][i])\n",
    "            matrix.append(row)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(A, B):\n",
    "        \"\"\"Inner-product of two matrices\"\"\"\n",
    "\n",
    "        return [[sum(ele_a * ele_b for ele_a, ele_b in zip(row_a, col_b)) for col_b in list(zip(*B))] for row_a in A]\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose(A):\n",
    "        \"\"\"Transpose a matrix\"\"\"\n",
    "\n",
    "        return [list(i) for i in zip(*A)]\n",
    "\n",
    "\n",
    "def pomdp_value_iteration(pomdp, epsilon=0.1):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "\n",
    "    U = {'': [[0] * len(pomdp.states)]}\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        prev_U = U\n",
    "        values = [val for action in U for val in U[action]]\n",
    "        value_matxs = []\n",
    "        for i in values:\n",
    "            for j in values:\n",
    "                value_matxs.append([i, j])\n",
    "\n",
    "        U1 = defaultdict(list)\n",
    "        for action in pomdp.actions:\n",
    "            for u in value_matxs:\n",
    "                u1 = Matrix.matmul(Matrix.matmul(pomdp.t_prob[int(action)],\n",
    "                                                 Matrix.multiply(pomdp.e_prob[int(action)], Matrix.transpose(u))),\n",
    "                                   [[1], [1]])\n",
    "                u1 = Matrix.add(Matrix.scalar_multiply(pomdp.gamma, Matrix.transpose(u1)), [pomdp.rewards[int(action)]])\n",
    "                U1[action].append(u1[0])\n",
    "\n",
    "        U = pomdp.remove_dominated_plans_fast(U1)\n",
    "        # replace with U = pomdp.remove_dominated_plans(U1) for accurate calculations\n",
    "\n",
    "        if count > 10:\n",
    "            if pomdp.max_difference(U, prev_U) < epsilon * (1 - pomdp.gamma) / pomdp.gamma:\n",
    "                return U\n",
    "\n",
    "#__doc__ += \n",
    "\"\"\"\n",
    ">>> pi = best_policy(sequential_decision_environment, value_iteration(sequential_decision_environment, .01))\n",
    ">>> sequential_decision_environment.to_arrows(pi)\n",
    "[['>', '>', '>', '.'], ['^', None, '^', '.'], ['^', '>', '^', '<']]\n",
    ">>> from utils import print_table\n",
    ">>> print_table(sequential_decision_environment.to_arrows(pi))\n",
    ">   >      >   .\n",
    "^   None   ^   .\n",
    "^   >      ^   <\n",
    ">>> print_table(sequential_decision_environment.to_arrows(policy_iteration(sequential_decision_environment)))\n",
    ">   >      >   .\n",
    "^   None   ^   .\n",
    "^   >      ^   <\n",
    "\"\"\"  # noqa\n",
    "\n",
    "\"\"\"\n",
    "s = { 'a' : {\t'plan1' : [(0.2, 'a'), (0.3, 'b'), (0.3, 'c'), (0.2, 'd')],\n",
    "                'plan2' : [(0.4, 'a'), (0.15, 'b'), (0.45, 'c')],\n",
    "                'plan3' : [(0.2, 'a'), (0.5, 'b'), (0.3, 'c')],\n",
    "                 },\n",
    "      'b' : {\t'plan1' : [(0.2, 'a'), (0.6, 'b'), (0.2, 'c'), (0.1, 'd')],\n",
    "                'plan2' : [(0.6, 'a'), (0.2, 'b'), (0.1, 'c'), (0.1, 'd')],\n",
    "                'plan3' : [(0.3, 'a'), (0.3, 'b'), (0.4, 'c')],\n",
    "                },\n",
    "        'c' : {\t'plan1' : [(0.3, 'a'), (0.5, 'b'), (0.1, 'c'), (0.1, 'd')],\n",
    "                'plan2' : [(0.5, 'a'), (0.3, 'b'), (0.1, 'c'), (0.1, 'd')],\n",
    "                'plan3' : [(0.1, 'a'), (0.3, 'b'), (0.1, 'c'), (0.5, 'd')],\n",
    "                },\n",
    "    }\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bae495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed utilities for this policy iteration:\n",
      "[0.5094155954147008, 0.649586359613105, 0.7953622428927029, 1.0]\n",
      "[0.3985112545104691, None, 0.4864404559151057, -1.0]\n",
      "[0.2964665410943774, 0.2539605460927299, 0.34478839971672015, 0.12994247010553683]\n",
      "\n",
      "This is the optimal policy in arrow/graph format: \n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n",
      "Execution time of policy iteration (4 x 3) =  0.0020008087158203125\n",
      "\n",
      "This is the optimal policy determined by the policy iteration: \n",
      " {(0, 1): (0, 1), (1, 2): (1, 0), (2, 1): (0, 1), (0, 0): (0, 1), (3, 1): None, (2, 0): (0, 1), (3, 0): (-1, 0), (0, 2): (1, 0), (2, 2): (1, 0), (1, 0): (1, 0), (3, 2): None}\n"
     ]
    }
   ],
   "source": [
    "#time package imported for measuring time for policy iteration run.\n",
    "import time\n",
    "\n",
    "#start time initiated for the policy iteration run\n",
    "start = time.time()\n",
    "\n",
    "#policy_iteration is performed to determine the optimal policy for the given environment. \n",
    "pi2 = policy_iteration(sequential_decision_environment)\n",
    "\n",
    "#end time initiated for the policy iteration run.\n",
    "end_time = time.time()\n",
    "\n",
    "#Prints out the optimal policy determined by the policy iteration in an arrow/graph form.\n",
    "#The arrows are the suggested direction to move from that state and the periods are the terminal or \n",
    "#end of navigation.\n",
    "print(\"\\nThis is the optimal policy in arrow/graph format: \")\n",
    "print_table(sequential_decision_environment.to_arrows(pi2))\n",
    "\n",
    "#Prints out the execution time for the policy iteration (difference between the end time and the start time)\n",
    "print(\"Execution time of policy iteration (4 x 3) = \", end_time-start)\n",
    "\n",
    "#Prints out the optimal policy list that is returned. \n",
    "optimal_policy = pi2\n",
    "print(\"\\nThis is the optimal policy determined by the policy iteration: \\n\", optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995413db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start point in the environment:  (0, 0)\n",
      "Current position:  (0, 0)\n",
      "Utility for move = 1.0\n",
      "NEWSTATE OPTIONS:  [(0.8, (0, 1)), (0.1, (1, 0)), (0.1, (0, 0))]\n",
      "['ahead']\n",
      "NewState coord:  ['ahead']  ,  (0, 1)\n",
      "Direction vector =  [0, 1] \n",
      "\n",
      "State before change =  (0, 0)\n",
      "State after change =  (0, 1) \n",
      "\n",
      "Current position:  (0, 1)\n",
      "Utility for move = 1.9\n",
      "NEWSTATE OPTIONS:  [(0.8, (0, 2)), (0.1, (0, 1)), (0.1, (0, 1))]\n",
      "['ahead']\n",
      "NewState coord:  ['ahead']  ,  (0, 2)\n",
      "Direction vector =  [0, 1] \n",
      "\n",
      "State before change =  (0, 1)\n",
      "State after change =  (0, 2) \n",
      "\n",
      "Current position:  (0, 2)\n",
      "Utility for move = 2.71\n",
      "NEWSTATE OPTIONS:  [(0.8, (1, 2)), (0.1, (0, 1)), (0.1, (0, 2))]\n",
      "['ahead']\n",
      "NewState coord:  ['ahead']  ,  (1, 2)\n",
      "Direction vector =  [1, 0] \n",
      "\n",
      "State before change =  (0, 2)\n",
      "State after change =  (1, 2) \n",
      "\n",
      "Current position:  (1, 2)\n",
      "Utility for move = 3.439\n",
      "NEWSTATE OPTIONS:  [(0.8, (2, 2)), (0.1, (1, 2)), (0.1, (1, 2))]\n",
      "['ahead']\n",
      "NewState coord:  ['ahead']  ,  (2, 2)\n",
      "Direction vector =  [1, 0] \n",
      "\n",
      "State before change =  (1, 2)\n",
      "State after change =  (2, 2) \n",
      "\n",
      "Current position:  (2, 2)\n",
      "Utility for move = 4.0951\n",
      "NEWSTATE OPTIONS:  [(0.8, (3, 2)), (0.1, (2, 1)), (0.1, (2, 2))]\n",
      "['ahead']\n",
      "NewState coord:  ['ahead']  ,  (3, 2)\n",
      "Direction vector =  [1, 0] \n",
      "\n",
      "State before change =  (2, 2)\n",
      "State after change =  (3, 2) \n",
      "\n",
      "Current position:  (3, 2)\n",
      "Utility for move = 4.68559\n",
      "Final cumulative utility for trial =  4.68559\n"
     ]
    }
   ],
   "source": [
    "#Pulls terminal states for the environment from the grid.\n",
    "terminals = sequential_decision_environment.terminals\n",
    "\n",
    "#TEST: print the list of terminal states in the environment.\n",
    "#print(\"List of terminals: \", terminals, '\\n')\n",
    "\n",
    "#Initiates the starting state and the state variable\n",
    "#THIS STARTING STATE CAN BE CHANGED THERE WILL BE AN ERROR THROWN IF YOU INPUT THE LOCATION OF THE OBSTACLE\n",
    "starting_state = (0,0)\n",
    "state = starting_state\n",
    "\n",
    "#Prints the initial start point for the environment.\n",
    "print(\"Start point in the environment: \", starting_state)\n",
    "\n",
    "#Initiates the cumulative utility, move, and the end variable that will serve as the end of the while loop\n",
    "move=0\n",
    "utility_cumulative = 0\n",
    "end = 0\n",
    "\n",
    "#Discount value for reward which is the gamma value of the MDP (previously generated in original initiation code)\n",
    "discount = sequential_decision_environment.gamma\n",
    "\n",
    "#While loop that is used for the simulation of the navigation. \n",
    "while end != 1:\n",
    "    \n",
    "    #TEST prints out the current state before any manipulation.\n",
    "    print(\"Current position: \", state)\n",
    "    \n",
    "    #TEST obtains the reward of the state from the environment and prints it.\n",
    "    #reward = sequential_decision_environment.R(state)\n",
    "    #print(\"Reward for this position = \", reward)\n",
    "    \n",
    "    #Calculates the utility for the current move given the move and the state reward. \n",
    "    utility_calc = (discount**move) * reward\n",
    "    \n",
    "    #Adds the current utility for the move to the cumulative utility for the trial and prints the \n",
    "    #current cumulative utility.\n",
    "    utility_cumulative += utility_calc\n",
    "    print(\"Utility for move =\", utility_cumulative)\n",
    "    \n",
    "    #Changes to the next move\n",
    "    move+=1\n",
    "    \n",
    "    #Checks if the current state is a terminal state. If so the navigation is terminated. If not\n",
    "    #the navigation continues. \n",
    "    if state in terminals:\n",
    "        end = 1\n",
    "    else:\n",
    "        #The possible new states moves for the current state are returned. \n",
    "        #This means that the states that can be moved to from the current state by only moving a single unit over\n",
    "        #are captured in this variable.\n",
    "        newstate = sequential_decision_environment.calculate_T(state, optimal_policy[state])\n",
    "        \n",
    "        #TEST: Prints the new state options availble for the current state to check that this is\n",
    "        #being performed properly. \n",
    "        print(\"NEWSTATE OPTIONS: \", newstate)\n",
    "        \n",
    "        #Chooses a random direction from the choices 'ahead', 'turn_right', and 'turn_left' and prints the selected direction. \n",
    "        #These directions are with respect to the newstates possible. So if I am moving from (0,2) ahead would mean move to (1,2),\n",
    "        #turn_right would mean to move to (0,1), and turn_left would mean to stay in the current state since a wall would be hit.\n",
    "        direction_to_move =random.choices(['ahead', 'turn_right','turn_left'], k=1, weights = [0.8, 0.1, 0.1])\n",
    "        print(direction_to_move)\n",
    "        \n",
    "        #Using the direction chosen randomly, the appropriate state coordinates are obtained from the new state options.\n",
    "        #The 'ahead' direction would be represented in the first option ([0]), the 'turn_right' direction would be represented\n",
    "        #in the second option [1], and the 'turn_left' direction would be represented in the third option [2]. \n",
    "        if direction_to_move == ['ahead']:\n",
    "            newstate_coord = newstate[0][1]\n",
    "        elif direction_to_move == ['turn_right']:\n",
    "            newstate_coord = newstate[1][1]\n",
    "        elif direction_to_move == ['turn_left']:\n",
    "            newstate_coord = newstate[2][1]\n",
    "            \n",
    "        #The newstate coordinate is returned as well as the direction the move is occuring.\n",
    "        print(\"NewState coord: \",direction_to_move, \" , \", newstate_coord)\n",
    "        \n",
    "        #Disects the newstate coordinate into the x axis and y axis coor respectively.\n",
    "        direction_coord_x = newstate_coord [0]\n",
    "        direction_coord_y = newstate_coord [1]\n",
    "        \n",
    "        #Disects the current state coordinate into the x axis and y axis coor respectively.\n",
    "        state_coord_x = state[0]\n",
    "        state_coord_y = state[1]\n",
    "       \n",
    "        #Takes the difference between the x coordinates for the current state and the newstate as well as the y coord.\n",
    "        #This is done to determine the amount and which axis the state needs to move. Without this, the amount of the move \n",
    "        #cannot be determined. DIRECTION VECTOR DETERMINATION\n",
    "        new_direction_coord_1 = direction_coord_x - state_coord_x\n",
    "        new_direction_coord_2 = direction_coord_y - state_coord_y\n",
    "        \n",
    "        #TEST: returns the direction vector\n",
    "        direction_vector=[new_direction_coord_1,new_direction_coord_2]\n",
    "        print(\"Direction vector = \", [new_direction_coord_1,new_direction_coord_2], \"\\n\")\n",
    "        \n",
    "        #TEST: returns the current state before the change is made.\n",
    "        print(\"State before change = \", state)\n",
    "        \n",
    "        #Performs the move from the current state to the newstate coordinate using the determined direction vector. \n",
    "        state = sequential_decision_environment.go(state, direction_vector)\n",
    "        \n",
    "        #TEST: returns the current state after the change is made.\n",
    "        print(\"State after change = \", state, '\\n') \n",
    "        \n",
    "        #TEST: Troubleshooting using the optimal policy states to ensure that the code is working properly.\n",
    "        #IF THIS IS USED IT WILL BE IDENTICAL TO THE POLICY UTILITIES SINCE IT IS ONLY USING THIS TO DO THE MOVES NO RANDOM ELEMENT\n",
    "        #state = sequential_decision_environment.go(state, optimal_policy[state])\n",
    "\n",
    "#The final cumulative utility for the trial is returned. \n",
    "print (\"Final cumulative utility for trial = \", utility_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703712de",
   "metadata": {},
   "source": [
    "# BELOW ARE THE SAMPLE ENVIRONMENTS MADE FOR THE EXECUTION TIMING EXPERIMENTS.\n",
    "\n",
    "There is a 6x5, 8x6, 12x9, and the original 4x3 environment here. Each of these are tested for execution time for the policy_iteration step only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23cb9c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed utilities for this policy iteration:\n",
      "[0.5094155954147008, 0.649586359613105, 0.7953622428927029, 1.0]\n",
      "[0.3985112545104691, None, 0.4864404559151057, -1.0]\n",
      "[0.2964665410943774, 0.2539605460927299, 0.34478839971672015, 0.12994247010553683]\n",
      "\n",
      "This is the optimal policy in arrow/graph format: \n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n",
      "Execution time of policy iteration (4 x 3) =  0.002000570297241211\n"
     ]
    }
   ],
   "source": [
    "#4x3\n",
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                                           [ -0.04, None, -0.04, -1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(3, 2), (3, 1)])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pi2 = policy_iteration(sequential_decision_environment)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nThis is the optimal policy in arrow/graph format: \")\n",
    "print_table(sequential_decision_environment.to_arrows(pi2))\n",
    "\n",
    "print(\"Execution time of policy iteration (4 x 3) = \", end_time-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ef3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed utilities for this policy iteration:\n",
      "[0.2768939980227997, 0.37870189615609606, 0.49651011848269505, 0.6344353865966916, 0.7954508331453506, 1.0]\n",
      "[0.21453525520398306, 0.30146044660563237, 0.3892303274403844, 0.49568446598152155, 0.4873362018029895, -1.0]\n",
      "[0.1395919313986193, None, 0.30121131462684114, 0.37618289503720714, 0.3575673468232874, 0.14254245524207035]\n",
      "[0.07960162679693036, 0.1325698864244548, 0.21339131323570956, 0.2731986325131458, 0.2571303321682535, 0.1677107281033714]\n",
      "[0.026714310341025053, 0.07774279018381222, 0.13724256845704105, 0.18448857567541205, 0.17148587894237213, 0.10831186633825407]\n",
      "\n",
      "This is the optimal policy in arrow/graph format: \n",
      ">   >      >   >   >   .\n",
      ">   >      ^   ^   ^   .\n",
      "^   None   ^   ^   ^   <\n",
      "^   >      ^   ^   ^   <\n",
      "^   >      ^   ^   ^   <\n",
      "Execution time of policy iteration (6 x 5) =  0.006999492645263672\n"
     ]
    }
   ],
   "source": [
    "#6x5\n",
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, -0.04, -0.04, +1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -1],\n",
    "                                           [-0.04, None, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [ -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(5, 4), (5, 3)])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pi3 = policy_iteration(sequential_decision_environment)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nThis is the optimal policy in arrow/graph format: \")\n",
    "print_table(sequential_decision_environment.to_arrows(pi3))\n",
    "\n",
    "print(\"Execution time of policy iteration (6 x 5) = \", end_time-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c84a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed utilities for this policy iteration:\n",
      "[0.11233370777620846, 0.18867518127787536, 0.27682995714021963, 0.37870189615609606, 0.49651011848269505, 0.6344353865966916, 0.7954508331453506, 1.0]\n",
      "[0.07086159506977144, 0.13752050913231587, 0.2138877307245626, 0.30146044660563237, 0.3892303274403844, 0.49568446598152155, 0.4873362018029895, -1.0]\n",
      "[0.0192977199405027, 0.0726730743958002, 0.1324612558431946, None, 0.30121129265109003, 0.37618267283794565, 0.35756489991835544, 0.14251548960875363]\n",
      "[-0.027098355514138597, 0.016068201388577043, 0.06869377577637073, 0.1319503716033199, 0.2133348452319306, 0.2731909008760787, 0.2571026706876008, 0.1674306851948642]\n",
      "[-0.06868207571791743, -0.03322081036805532, 0.0154607938531309, 0.0719305510225687, 0.13667137335317817, 0.18440594660174361, 0.1712008263242333, 0.10544853594448675]\n",
      "[-0.10515392744899849, -0.06932199401875107, -0.027907141144347426, 0.018351430710920456, 0.06975840604848188, 0.10781542744679931, 0.09738765943534011, 0.049107511240891084]\n",
      "\n",
      "This is the optimal policy in arrow/graph format: \n",
      ">   >   >   >      >   >   >   .\n",
      ">   >   >   >      ^   ^   ^   .\n",
      "^   ^   ^   None   ^   ^   ^   <\n",
      "^   ^   ^   >      ^   ^   ^   <\n",
      "^   ^   >   >      ^   ^   ^   ^\n",
      "^   >   >   >      ^   ^   ^   ^\n",
      "Execution time of policy iteration (8 x 6) =  0.008999109268188477\n"
     ]
    }
   ],
   "source": [
    "#8x6\n",
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, +1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -1],\n",
    "                                           [-0.04, -0.04, -0.04, None, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(7, 5), (7, 4)])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pi4 = policy_iteration(sequential_decision_environment)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nThis is the optimal policy in arrow/graph format: \")\n",
    "print_table(sequential_decision_environment.to_arrows(pi4))\n",
    "\n",
    "print(\"Execution time of policy iteration (8 x 6) = \", end_time-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0595d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed utilities for this policy iteration:\n",
      "[-0.1364786275489372, -0.0962772241071054, -0.04925625742954484, 0.008272862782585828, 0.11068052888687419, 0.1870926819592859, 0.2754339681291945, 0.3777316615380224, 0.4964146319286437, 0.6344260525335632, 0.795449993079669, 1.0]\n",
      "[-0.1652883301368539, -0.1349752065337761, -0.11977394960370844, -0.3573519518488478, 0.06680611418188503, 0.1326875947770015, 0.20753460766878684, 0.2924141890108545, 0.3883395247877812, 0.4955968087575647, 0.48732770780554263, -1.0]\n",
      "[-0.17714776979223334, -0.146363944267785, -0.11125468401511698, -0.06885457455819735, 0.014553314918058557, 0.07138150976821381, 0.13563704943715638, 0.20904312916922155, 0.2921953181340255, 0.3752997629065885, 0.3574720218786628, 0.1424343733008782]\n",
      "[-0.18766422452265413, -0.15680892432067692, -0.12110816520968026, -0.07968095147559065, -0.031710248793611555, 0.019788340483678647, 0.07164313050140045, 0.13559449181240868, 0.20706174517159007, 0.27197346502194364, 0.25691198515285313, 0.16727241720267771]\n",
      "[-0.209094441448528, -0.1826476709510448, -0.15249953984327183, -0.11836289593280519, -0.08075169204983837, None, 0.019778906714277053, 0.0712861238775977, 0.13197764631531658, 0.18308265158653272, 0.17093076224838663, 0.1052966032838272]\n",
      "[-0.22998153507469912, -0.20816887861142402, -0.1819835544753303, -0.15228646229363196, -0.11874063688035172, -0.08049347240325538, -0.03170726528094468, 0.014403715567431377, 0.06590400278717837, 0.1064862559897955, 0.09705985107384318, 0.04895488017692471]\n",
      "[-0.24866937830490615, -0.23002698892874626, -0.20680381153356436, -0.18035114691797352, -0.1501964940613597, -0.11577587649646842, -0.0764714306990214, -0.035804120133989044, 0.007862593559692155, 0.04037869294425279, 0.03334394679141931, -0.0019247594081169966]\n",
      "[-0.265313184537491, -0.2488116172172236, -0.2285829538871156, -0.20559877319297362, -0.17945784071855977, -0.14968926529866677, -0.11573778387598696, -0.0800702210645726, -0.043053933547351236, -0.016763122383533936, -0.0217880809348259, -0.04763379566810832]\n",
      "[-0.27704821750308944, -0.26143846016570194, -0.2437721572239171, -0.22397249614433734, -0.20182094708885884, -0.17709146692526256, -0.14955722364598406, -0.11900093467917588, -0.08766917093938904, -0.06622505182411419, -0.06961531470471927, -0.08852935297193706]\n",
      "\n",
      "This is the optimal policy in arrow/graph format: \n",
      ">   >   >   >   >   >      >   >   >   >   >   .\n",
      "^   ^   ^   >   >   >      >   >   ^   ^   ^   .\n",
      ">   >   >   >   >   >      >   ^   ^   ^   ^   <\n",
      ">   >   >   >   >   >      ^   ^   ^   ^   ^   <\n",
      ">   >   >   ^   ^   None   ^   ^   ^   ^   ^   ^\n",
      "^   >   >   ^   >   >      ^   ^   ^   ^   ^   ^\n",
      "^   >   >   >   >   >      ^   ^   ^   ^   ^   ^\n",
      "^   >   >   >   >   >      ^   ^   ^   ^   ^   ^\n",
      ">   >   >   >   >   >      >   ^   ^   ^   ^   ^\n",
      "Execution time of policy iteration (12 x 9) =  0.02399921417236328\n"
     ]
    }
   ],
   "source": [
    "#12x9\n",
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, +1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.40, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, None, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(11, 8), (11, 7)])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pi5 = policy_iteration(sequential_decision_environment)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nThis is the optimal policy in arrow/graph format: \")\n",
    "print_table(sequential_decision_environment.to_arrows(pi5))\n",
    "\n",
    "print(\"Execution time of policy iteration (12 x 9) = \", end_time-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
